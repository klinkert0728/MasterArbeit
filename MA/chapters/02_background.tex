\section{Background}
\label{cha:background}
Cloud benchmarking is a common way to study and analyze the quality and performance of cloud services based on experiments  \cite{bermbach2017quality, vazquez2014cloudBenchmarkSurvey}. However, every cloud environment has constraints such as shared resources among users, high costs, and a wide variety of Virtual Machines \cite{snellman2011towardsAutomatic, scheuner2018estimatingCloudApplication}. Despite these constraints, it can be effectively used by using benchmarking tools that create artificial workloads to measure its quality and collect information on states \cite{grambow2020benchmarkingMicroservicesApplication, grambow2019continuousBenchmarking, bermbach2017quality}. \\
Benchmarking aims to answer specific questions and compares different software versions, deployments, and configurations to report and evaluate how well a system under test can handle specific and predefined workloads under specific resource constraints. It aims to retrieve accurate and meaningful results to assess a system’s performance and reduce the uncertainty in a production release \cite{folkerts2012benchmarking, grambow2020benchmarkingMicroservicesApplication, grambow2019continuousBenchmarking, grambow2020benchmarkingMicroservicesperformance}. \\
Its objective is to offer developers a solution to compare the results of running different scenarios under the same conditions in a structured, repeatable, and measurable way to identify and evaluate performance changes and optimize them accordingly before deploying an application and consuming cloud resources \cite{shang2015automatedDetection}. \\
This method gives developers a detailed breakdown of how the application’s behavior will be under scenarios similar to those encountered after deployment \cite{palit2016demystifying, palit2016demystifying}. Additionally, benchmarking can simulate scenarios under extreme conditions to uncover potential failures before they manifest in production, reducing downtime, ensuring a smoother user experience, minimizing costs, and ensuring scalability \cite{japke2023earlymicrobenchmarkcatches, grambow2020benchmarkingMicroservicesperformance, grambow2020benchmarkingMicroservicesApplication}. However, benchmarking can be costly. To mitigate costs, the developer should constantly monitor and analyze the results to identify if the requirements have been met \cite{grambow2020benchmarkingMicroservicesperformance}. \\
There are two main benchmarking techniques: application benchmarks and microbenchmarks. Although they are different techniques, they share the same general objective: to identify and test the performance of an environment using a system under test to simulate realistic workloads and real-world usage \cite{japke2023earlymicrobenchmarkcatches, grambow2021usingApplication, grambow2020benchmarkingMicroservicesperformance}. These techniques help developers understand how the system will behave under typical or extreme conditions, allowing them to rely on quantifiable and insightful data to assess its capabilities and enhance their decision-making. 

\subsection{Application Benchmark}
\label{sec:application_benchmarks}
This benchmarking technique tests the entire system. It deploys the \ac{SUT} into an environment similar to production by generating artificial loads into the system to simulate real-world conditions. It evaluates how the application behaves under an established atmosphere by measuring response time, throughput, and scalability \cite{denaro2004early}. Application benchmarks are often preferred when the goal is to understand how well a cloud platform supports the application's performance \cite{grambow2021usingApplication}. A client generally generates the load of the application's benchmark to avoid using the \ac{SUT}'s resources to develop fake loads; therefore, the \ac{SUT} is like a black box where only its \ac{API}s are used to stress the system \cite{japke2023earlymicrobenchmarkcatches}. It aims to help developers understand and evaluate the application's performance under expected stress scenarios and how well the system under test can handle artificial workloads. \\
However, replicating a complete production environment for application benchmarking is complex and time-consuming. Developers may face several challenges when doing application benchmarks. On the one hand, they must make critical decisions in advance, including selecting the appropriate machine configuration, network setup, and storage capacity. These decisions will impact the accuracy of the results. Additionally, multiple external factors can affect the performance of the \ac{SUT} when running the application benchmark on the cloud \cite{denaro2004early}. As Bulej et al. \cite{bulej2019initial} stated, the resources in the cloud could be shared between multiple applications, affecting the \ac{SUT}'s available resources. Similarly, depending on the machine type, the cloud provider might boost the machine's performance, affecting the end measurable results, which can introduce variability in the results and make it challenging to obtain consistent, reliable, and repeatable performance data. Therefore, as Grambow et al. \cite{grambow} stated, running this benchmark multiple times is recommended to obtain meaningful resources, leading to higher costs and the use of computer resources. To address these challenges, developers tend to implement techniques like duet benchmarking, which reduces the impact of external noise in shared environments by ensuring that performance results are not affected by these external factors \cite{bulej2020duet}.

\subsubsection{Duet Benchmarking}
\label{sec:duet_benchmarking}
As previously stated, the client executes artificial workloads in a shared cloud environment. This shared environment often leads to performance variability caused by external noise, which refers to any factor or situation outside the system under test that can affect or influence its performance, which might affect the benchmark’s overall performance \cite{japke2023earlymicrobenchmarkcatches, bulej2020duet}. \\
Bulej et al. \cite{bulej2019initial} argue in their research that duet benchmarking is a recommended technique to address this challenge, as external noise can compromise the reliability and accuracy of benchmark results. \\
Duet benchmarking measures the results obtained by two workloads executed in parallel in the same virtual machine. Both workloads are exposed to the same external factors since they share the same computational environment and resources \cite{bulej2019initial, bulej2020duet, grambow}. Duet benchmarking guarantees that any external influences, for example, other workloads that affect the cloud or shared resources, will have an equal impact on both workloads instead of only one; thus, making a fair comparison of the performance results of the workloads and the same result should be obtained \cite{bulej2019initial, bulej2020duet}. \\
The key feature of duet benchmarking is its ability to neutralize and reduce the impact of external factors on performance assessment \cite{bulej2019initial}. Since both workloads experience the same conditions, any observed differences in performance can be attributed directly to the system under test rather than the environmental noise, making performance results more accurate and reliable \cite{bulej2019initial, bulej2020duet}. Moreover, duet benchmarking helps developers identify and highlight meaningful differences between the compared workloads. Since both workloads operate under identical conditions, the technique effectively isolates performance changes related to the system rather than external factors, ensuring that these differences are evaluated under the same consistent and realistic operating conditions \cite{bulej2020duet}. Therefore, this technique provides a more precise assessment of the \ac{SUT} performance improvements or regressions, preventing developers from being misled by external noise \cite{japke2023earlymicrobenchmarkcatches}. \\
Lastly, this technique prevents developers from being misled by external noise by reducing bias by standardizing the testing environment for both workloads; duet benchmarking ensures consistent and realistic operating conditions. This consistency reduces the variability in results, enabling developers to trust the performance data and make informed decisions about optimizing the application \cite{bulej2020duet, bulej2019initial, japke2023earlymicrobenchmarkcatches}.

\subsection{Microbenchmark}
\label{sec:microbenchmark}
Opposite to application benchmarks, which focus on evaluating the entire system's performance under real-world scenarios, microbenchmarks focus only on testing individual and specific functions of the \ac{SUT} without considering the context of the entire application \cite{gil2011microbenchmarkleasonslearned, laaber2019continuous}. These microbenchmarks evaluate the \ac{SUT} by repeatedly calling the particular function under test with artificial workloads for a specific duration and number of iterations \cite{japke2023earlymicrobenchmarkcatches, grambow, laaber2019softwaremicrobenchmarkinginthecloud, bermbach2017benchfoundry}. This specific scope allows microbenchmarks to provide detailed insights into what is happening with the performance behavior in specific functions of the system under test. However, as Gil et al. \cite{gil2011microbenchmarkleasonslearned} argued, developers can draw wrong or inconclusive conclusions from the experiment if a microbenchmark suite is not executed correctly using the appropriate functions.  \\
Microbenchmarks detect performance issues on specific functions and resources. They allow developers to debug performance bottlenecks by evaluating code changes' impact on an isolated function without considering the external environment or the entire application \cite{scheuner2018estimatingCloudApplication, seltzer1999thecaseforapplicationspecific}. Therefore, microbenchmarks help understand the raw capabilities of specific resources or functions by measuring how the system will react to small workloads when testing isolated features \cite{bermbach2017benchfoundry, gil2011microbenchmarkleasonslearned}. \\
Microbenchmarks serve as the building blocks for predicting the behavior and performance of the entire application \cite{grambow}. Moreover, microbenchmarks allow developers to isolate components and fine-tune them, improving their functionalities and mitigating poor performance that could impact the entire application’s behavior. This predictive power gives developers confidence in their testing strategies and the ability to address potential performance issues \cite{scheuner2018estimatingCloudApplication, grambow2021usingApplication} proactively.\\
Microbenchmarks are relatively more straightforward than application benchmarks as they require a minimal setup \cite{japke2023earlymicrobenchmarkcatches}. Therefore, they take less time to set up and execute, making them less resource-intensive and faster to run \cite{grambow}. Since they are easier to set up than application benchmarks, developers tend to include them more often in the \ac{CI}/\ac{CD} pipelines to measure and provide insights on performance metrics such as \ac{CPU} usage, I/O throughput, latency memory consumption, execution time, storage, and network performance \cite{scheuner2018cloudBenchmarkingSuit, laaber2024evaluatingSeachbasedsoftware, grambow2021usingApplication, scheuner2018estimatingCloudApplication, laaber2021applyingtcp, sun2022dissecting}. \\
Frequent testing helps developers get faster warnings on how the function behaves without waiting for the whole application to be deployed. Additionally, running microbenchmarks often and throughout the development process leads developers to identify performance issues and bottlenecks early \cite{schirmer2024elastibench}. However, given that a \ac{SUT} has multiple functions when designing a microbenchmark suite, it is required to determine and carefully select which functions to benchmark to avoid redundancy and ensure resource allocation and reliable results. As Scheuner and Leitner \cite{scheuner2018cloudBenchmarkingSuit} argues, a prior benchmark selection helps avoid running similar microbenchmarks that will bring similar results and avoid undesirable testing, ensuring more efficient use and allocation of resources. Not having this prior careful selection can lead developers to run similar microbenchmarks that lead to undesirable duplicating results that provide unreliable insights about the function's performance, leading to poor testing practices and waste of resources. \\
Similarly to application benchmarks, running multiple microbenchmarks from the same suite is recommended to increase the likelihood of getting reliable results \cite{gil2011microbenchmarkleasonslearned}. However, despite their result precision, microbenchmarks face a limitation. Since the microbenchmarks' scope is limited, similar to unit tests, it is hard to know if the outcome of the microbenchmarks will have a real improvement or downgrade in the performance of the production environment. They might not reflect the impact in the whole tested environment, given that they solely focus on isolated functions while ignoring other parts of the application \cite{grambow, grambow2021usingApplication}. \\
In their research, Scheuner and Leitner \cite{scheuner2018cloudBenchmarkingSuit} suggest implementing different techniques to get more accurate results and reduce bias when making microbenchmarks. One of the most common techniques is randomized multiple interleaved trials, further referred to as \ac{RMIT}, which shuffles the execution order of workloads across iterations  \cite{japke2023earlymicrobenchmarkcatches, scheuner2018cloudBenchmarkingSuit}. This technique decreases the probability of having the same results by running the same artificial workloads in the same way all the time \cite{abedi2017conductingrepeatable}. By introducing randomness, \ac{RMIT} enhances the reliability of microbenchmark results and ensures that the insights obtained on the isolated functions are robust and actionable. 

\subsubsection{Randomized Multiple Interleaved Trials}
\label{sec:randomizedMultipleInterleavedTrials}
Considering the best practices for getting more accurate and unbiased results and avoiding the SUT of expecting a specific workload order, Scheuner and Leitner \cite{scheuner2018cloudBenchmarkingSuit} recommend in their research that microbenchmark suites be tested using Randomized Multiple Interleaved Trials (\ac{RMIT}). This technique addresses potential biases by avoiding predictable execution orders in the SUT, ensuring the performance measures are reliable and stable \cite{scheuner2018cloudBenchmarkingSuit, abedi2017conductingrepeatable}. It runs the microbenchmarks multiple times on the same cloud \ac{VM}s, shuffling their execution order each time by reorganizing the test's order and repeating the execution of the microbenchmark suite multiple times while keeping them as realistic an environment as possible, reducing the influence of external noise  \cite{japke2023earlymicrobenchmarkcatches,scheuner2018cloudBenchmarkingSuit}, while ensuring that the runs are not affected by the environmental changes \cite{abedi2017conductingrepeatable}. \\
Shuffling the artificial workloads in every iteration prevents the system under test from adapting or learning specific patterns of the instances, which could lead to unreliable results \cite{abedi2017conductingrepeatable, scheuner2018cloudBenchmarkingSuit}. Furthermore, this technique ensures that performance measurements remain reliable, even in dynamic and non-controlled environments \cite{abedi2017conductingrepeatable, scheuner2018cloudBenchmarkingSuit}. However, even though it reduces bias and ensures more accurate performance measurement, this technique has a limitation as it increases the execution time of the experiment, leading to more usage of resources, time, and costs  \cite{japke2023earlymicrobenchmarkcatches}. \\
The main advantage of \ac{RMIT} is its ability to mitigate the effects of unpredictable factors that might affect the microbenchmark's performance measurements. These factors are external sources that developers cannot control; the most common are resource and infrastructure fluctuations \cite{abedi2017conductingrepeatable}. To deal with these factors \ac{RMIT} distributes the impact of these external influences across a broader dataset by randomizing workload execution and conducting multiple trials, reducing their impact and influence in the final results \cite{abedi2017conductingrepeatable}. This randomization enables developers to fairly compare the results obtained in the microbenchmark and reduce bias, decreasing the probability of the same instance getting affected by external noise every time it runs. Therefore, \ac{RMIT} helps developers mitigate these external influences, providing a more robust, reliable, and unbiased performance measure while ensuring that the results will be based on a real-case scenario and not on a single instance. Thus, \ac{RMIT} helps developers confidently identify, detect, and address anomalies and performance failures.

\subsection{Continuous integration and Continuous deployment pipelines}
\label{sec:continuousIntegrationAndContinousDeplymentPipelines}
As microbenchmarks take less time to execute than application benchmarks, developers tend to integrate microbenchmarks into their application deployment pipelines using standard practices known as continuous integration, further referred to as \ac{CI}, and continuous deployment, further referred to as \ac{CD}, to help continuously evaluate all code changes, which helps to identify and detect failures as early as possible. However, given that application benchmarks require more time to deploy and a more complex setup, developers who attempt to include them in their \ac{CI}/\ac{CD} pipelines will have to wait a long time to see the results. Therefore, using microbenchmarks in the \ac{CI}/\ac{CD} pipelines is a more common approach. \\
The \ac{CI}/\ac{CD} pipeline aims to improve and shorten an application's release cycle of new features, detect early failures, and increase productivity while maintaining high-quality code standards \cite{laaber2018evaluationofopensourcesoftware, zampetti2021cicdpipelinesevulution, joshi2022implementingAutomatedTesting, rangnau2020continuoussecuritytesting}. Manual testing makes developers fall behind in the application's deployment as its complexity increases. For this reason, as Joshi \cite{joshi2022implementingAutomatedTesting} stated in his research, both \ac{CI} and \ac{CD} play important roles at different stages of the deployment process to make it faster and more reliable while making tests automatic and requiring less human intervention. \\
Continuous integration (\ac{CI}) is present in the early stage as it is a practice where developers often integrate code changes in the shared repository \cite{zampetti2021cicdpipelinesevulution, goyal2024optimisingcloudbasedcicd, laaber2018evaluationofopensourcesoftware}. Developers build automated tests into the application to control and verify every integration and detect performance regression and early failures. This approach helps developers ensure that every integrated change or update will be tested automatically to facilitate failure detection and poor performance quickly, reducing integration problems \cite{zampetti2021cicdpipelinesevulution, goyal2024optimisingcloudbasedcicd, rangnau2020continuoussecuritytesting}. \\ 
On the other hand, continuous deployment (\ac{CD}) is the subsequent stage of \ac{CI}, which happens after the integration phase, where all changes successfully pass the test \cite{rangnau2020continuoussecuritytesting}. It aims to accomplish short-cycle releases by continuously deploying new changes or features to a production environment \cite{rangnau2020continuoussecuritytesting, zampetti2021cicdpipelinesevulution}. This approach focuses on automating the release process of the integrated changes to enable the application's deployment and ensure that end users have the latest version of the application as fast as possible \cite{goyal2024optimisingcloudbasedcicd}. \\
Together, both approaches create a strong pipeline that enhances software quality, accelerates the deployment cycle, and reduces the need for developers' manual testing and intervention, as by automating these tasks, developers can focus their time and resources on other aspects \cite{joshi2022implementingAutomatedTesting, zampetti2021cicdpipelinesevulution}. Furthermore, \ac{CI}/\ac{CD} pipelines improve overall system response and reliability by providing immediate feedback on code changes, detecting and addressing failures and performance bugs, and regressions before deployment to ensure that production systems remain stable and performant \cite{goyal2024optimisingcloudbasedcicd, dimitrov2000impact, joshi2022implementingAutomatedTesting}.

\subsection{Ridge Regression}
When having multicollinearity in the variables, a ridge regression model is a commonly used statistical method that implements an L2 regularization to penalize collinearity and improve data generalization \cite{mcdonald2009ridgeregression}. Its objective is to assess the changes of the dependent variables concerning the independent variables to ensure that the model is not over-sensitive to minor data fluctuations to avoid poor model generalization \cite{mcdonald2009ridgeregression}. It uses two statistical metrics to test the accuracy of the model. The coefficient of determination is further referred to as R-squared \cite{chicco2021coefficientofdetermination}, and the mean square error is further referred to as \ac{MSE} \cite{wang2009meansquareerror}. The R-squared measures how much the independent variable determines the dependent variable. A higher R-squared indicates a better fit of the model \cite{chicco2021coefficientofdetermination, onyutharfromrsquared, gao2024rrquaredhowmuch}. However, a high R-squared can also indicate overfitting, where the model captures noise rather than relevant data; hence, it is not learning general trends. The \ac{MSE} a direct measure of the model's predictive accuracy. It aims to measure the average squared difference between observed and predicted values. Opposite to R-squared, the \ac{MSE} is more sensitive to outliers as they significantly contribute to the final result. Therefore, a lower \ac{MSE} indicates better predictive accuracy where the model's predictions are closely aligned with the actual values \cite{chicco2021coefficientofdetermination, wang2009meansquareerror}.
For this reason, it is important to analyze both R-squared and \ac{MSE} results when assessing the ridge regression model. A higher R-squared and lower \ac{MSE} better fit the data, balancing the tradeoff between bias and variance \cite{chicco2021coefficientofdetermination}.

