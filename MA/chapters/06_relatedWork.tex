\section{Related Work}
\label{cha:relatedWork}
A lot of research focuses on key areas when discussing benchmarking. This research considers the following related work areas to perform the experiments and analyze the results. We focus on two main sections: balancing accuracy and execution time and cloud benchmarking, as they provide the fundamental knowledge needed to perform the experiments. 

\subsection{Balancing accuracy and Execution Time}
In benchmarking, one of the challenges developers face is the tradeoff between costs and execution time \cite{japke2023earlymicrobenchmarkcatches, grambow, grambow2021usingApplication, alghamdi2023towards, laaber2021applyingtcp, he2019statistics, grambow2020benchmarkingMicroservicesperformance}. The longer developers execute the benchmark, the higher the costs; however, if the benchmark is not executed enough, it will lead to inaccurate results. Therefore, a balance is needed. Besides the approach used in this study, researchers use other approaches that focus on reducing the benchmark execution. Approaches like implementing test case prioritization \cite{laaber2021applyingtcp, laaber2019continuous, mostafa2017perfrankerprioritization, laaber2022multi}, stopping the benchmark when the conditions have high confidence of getting accurate results \cite{he2019statistics}, reducing redundancies by optimizing microbenchmarks \cite{grambow, grambow2021usingApplication, de2017perphecyperformance}, applying a principal component analysis \ac{PCA} \cite{trancoso2005reducingTPC} are commonly used when researching reduction of benchmarking execution times.\\
Laaber et al. \cite{laaber2021applyingtcp} explore using software microbenchmarks in performance testing pipelines using a test case prioritization \ac{TCP} approach. Demonstrating that coverage-based prioritization can be applied to microbenchmarks, allowing performance testers to schedule and run the most important benchmarks first. Laaber et al. \cite{laaber2021applyingtcp} use \ac{TCP} strategies in functional testing but adapt them to handle the continuous, distribution-based results common in performance evaluations. Notably, Laaber et al. \cite{laaber2021applyingtcp} found that prioritizing benchmarks by either their total coverage (how many project methods a benchmark touches) or additional coverage (the unique methods each benchmark tests beyond what has already been covered) can detect significant performance regressions earlierâ€”thereby lowering the time needed to receive actionable feedback. The fewer microbenchmarks we run, the faster the suite will complete; therefore, the costs will be reduced. Thus, removing redundant tests and running only the most important tests saves time and costs while performing the benchmark suite without compromising the results. Therefore, This approach has the potential to streamline model training and improve performance estimates, ultimately providing faster, more precise feedback during the software development cycle.  \\
Japke et al. \cite{japke2023earlymicrobenchmarkcatches} created a \ac{SUT} using a microservice application testbed application, evaluating a flight booking service using application benchmarks and microbenchmarks. In their study, they conclude that although microbenchmarks detect performance issues much earlier than application benchmarks, application benchmarks continue to be more effective due to the cost and time associated with running the full suite of microbenchmarks.   \\
In our study, we ran the experiments using the full microbenchmark suite in a \ac{SUT} used in production environments. While performing our benchmarks, one of the biggest challenges was that the microbenchmark suite changed between versions; as expected, some microbenchmarks were no longer necessary and thus were removed from the code, while adding some new ones; this level of complexity was removed entirely by not using a \ac{SUT} deployed in production. This microbenchmark suite change was the most significant disadvantage of the results presented by Japke et al. \cite{japke2023earlymicrobenchmarkcatches}. Nevertheless, Japke et al. \cite{japke2023earlymicrobenchmarkcatches} implemented \ac{RMIT} duet benchmark techniques to reduce the noise from the cloud, giving robustness to their results. We also follow those techniques, deploying different versions of \ac{SUT} seen in \cref{tab:version_time} aand then running the application benchmark and microbenchmarks. Although we did not deploy the same version to run the benchmarks, the idea of using these techniques is to reduce the influence of cloud variability on the \ac{SUT}. So if version v1.104 was affected by any external factor, the assumption is that v1.105 will also be affected as they were deployed in the same Virtual machine. In our study, we experienced a similar behavior where the entire run of the microbenchmark suite took around 8 hours to complete. \\ 
In contrast, the application benchmark duration was considerably less, with an average time of one hour and thirty minutes. However, Japke et al. \cite{japke2023earlymicrobenchmarkcatches} contributions clearly show that microbenchmarks detect performance issues faster despite executing them longer. Although our model could not provide a good prediction, we also see that the microbenchmark data shows a performance improvement during the comparison of the different pairs, except in one version, where both application benchmark and microbenchmark present a downgrade in performance. This strengthens the purpose of attempting to predict application benchmarks using microbenchmarks, as our data also shows that microbenchmark performance varies similarly to application performance when testing different versions. Lastly, using techniques like the one mentioned earlier, \ac{TCP} by Laaber et al. \cite{laaber2021applyingtcp} we can considerably reduce the time it takes to run the microbenchmark suite, making it more efficient without compromising the data results.  \\
Grambow et al. \cite{grambow2021usingApplication} propose another approach to reduce the time of the microbenchmark execution and increase its relevance by mapping microbenchmark suites to application call graphs to determine if a system under test covers with microbenchmarks the functions used by the application benchmark. Firstly, they run an application benchmark as a base and record the call graphs to capture which functions have been used; then, they compare the result of the call graph to the microbenchmark suite to determine the coverage from microbenchmarks of the functions the application benchmark has used. This approach not only improves the time execution time and the relevance of the microbenchmarks but also serves as a guide to the developers to identify which portions of the code have yet been tested and should be covered; therefore, it serves as a starting point on how to improve the reliability of the \ac{SUT}, although having the microbenchmark test cover the complete \ac{SUT}, it does not guarantee detection of performance issues. Nonetheless, having good test coverage for your system is always a good practice. One challenge in our research was that the microbenchmark suite was too big; with \ac{TCP}, we knew that not all microbenchmarks are equally important as there are sometimes redundancies between the microbenchmarks. Grambow et al. \cite{grambow2021usingApplication} fits perfectly as a pre-step execution where one can filter the microbenchmarks used by the application benchmark and, therefore, have a smaller subset as features, reducing the model complexity. In another research, Grambow et al. \cite{grambow} investigated further if an optimized microbenchmark suite detects the same performance changes as an application benchmark; they analyze two-time series databases where first they apply the technique about call graphs mentioned before to optimize the microbenchmark suite and then compare the results with the application benchmark, with this approach they conclude that there was not a clear benefit of using an optimized version of the microbenchmark suite as a proxy. However, it serves as alternative performance feedback for developers in specific situations. In our study, we can also confirm that for the ridge regression, it did not matter how many microbenchmarks were selected as features; it always yielded an inferior R-squared result, but we do not think these results are directly related to the number of microbenchmarks selected but instead with the similarity from the microbenchmarks, if further research address this topic, then the using leveraging the research by Grambow et al. \cite{grambow} we could have a model that at performs better when explaining the distribution of the data. \\
Trancoso et al. \cite{trancoso2005reducingTPC} propose yet another approach to reduce the time it takes to execute the benchmarks and argue that microbenchmarks are not able to capture the full details of an application; they select a subset of queries using a statistical method, principal component analysis \ac{PCA} to reduce the number of queries that are necessary to obtain an accuracy o \text{80\%} with only \text{20\%} f the execution time of the complete benchmark, hence reducing considerably the time it takes to execute the benchmarks. On the other hand, in our research, we can see that application benchmarks took less than the microbenchmarks to execute, so the time it takes to execute, although it could be improved, was not the main complexity of the application benchmark; for this research, the setup phase for these type of benchmark is the one that carries most of the weight, choosing the right machines, configuring networks, deploying components, and dealing with external factors. Furthermore, as stated by Japke et al. \cite{japke2023earlymicrobenchmarkcatches} we know that application benchmarks are more prone to detect false positives.  

\subsection{Cloud Benchmarking}
The reliability of benchmarks depends completely on the accuracy of their performance measurements; when multiple runs of the benchmarks are unstable, they add a high level of uncertainty for the validity of the results. This is specifically a challenge in cloud benchmarking as the variability of the cloud directly influences the results captured by the benchmark \cite{laaber2018evaluationofopensourcesoftware,folkerts2012benchmarking,bulej2019initial,bulej2020duet,bermbach2017quality,leitnerPatternsinthechaos}. Therefore, researchers recommend running the benchmarks multiple times to capture more reliable and accurate results \cite{japke2023earlymicrobenchmarkcatches, scheuner2018cloudBenchmarkingSuit, abedi2017conductingrepeatable, grambow}. \\
A few techniques can be implemented to reduce the impact of cloud variability on the results of the benchmarks. Bulej et al. \cite{bulej2020duet} suggest that duet benchmarking can be used in the execution of the benchmarks to perform a fair comparison between two \ac{SUT}s or two versions of the same \ac{SUT}. Our research follows this approach and the approach taken by Grambow et al. \cite{grambow} in the execution of our multiple benchmarks; we used this technique when running the application benchmarks, which allowed us to reduce the uncertainty when we were performing the application benchmarks. Furthermore, our research analyses a pair of versions of Victoria metrics; although we can not see a clear performance improvement or downgrade in \cref{fig:mean_query_rate}, we can see that there is some variability as expected in the data; the important part is that as we see in both \cref{fig:mean_query_rate,fig:insertTime}, they both seem to increase or decrease with a certain relationship, although we can not say for certain that this is because of external factors in the cloud environment if we would not have implemented duet benchmarking, this variability could have misled the research to think there was an improvement or downgrade in the \ac{SUT}, but it was only an external factor influencing the results of the benchmark.   