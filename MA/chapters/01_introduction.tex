\section{Introduction}
\label{cha:introduction}

Modern development practices no longer focus solely on testing and ensuring the application's functionality. There is a growing emphasis and interest in evaluating and measuring applications’ performance to gain deeper insights into their efficiency, responsiveness, and reliability \cite{chen2017exploratory, ameller2012software}. By analyzing performance, developers can understand how users interact with applications, make data-driven decisions, and optimize resource usage \cite{chen2017exploratory, bulej2019initial, gunningberg1989application, ameller2012software}. \\ 
There are three main ways to characterize an application’s performance: latency, scalability, and throughput \cite{denaro2004early, laaber2019continuous}. Research defines latency as the delay between the operation and the executed action \cite{dimitrov2000impact, denaro2004early}. Scalability refers to the dependency between the number of resources a system can handle without excessive additional costs \cite{denaro2004early, bondi2000characteristics}. Lastly, research describes throughput as the response rate of the number of actions and operations that can be completed under a certain period \cite{r2006throughput, denaro2004early}. Any change in any of them may result in a performance issue that affects the application’s overall behavior.  \\
Performance is an issue that can affect both developers and the end user, affecting customer satisfaction and determining the application’s success. On the one hand, it affects end users given the fact that performance issues affect the overall behavior of the application and tend to have a significant impact on user experience, driving users away, discouraging engagement, decreasing the application’s responsiveness, and undermining the application’s reputation \cite{grambow, alghamdi2023towards}. On the other hand, for developers, these issues are more than just inconveniences or bugs; they represent costs, time investment, and potential delays in upcoming releases.  \\
Developers detect performance issues as either performance bugs or regressions. Its early detection reduces and mitigates escalating problems that can impact the application more. Performance bugs refer to programming errors that cause inefficient application operation and tend to create functional bugs and crashes \cite{nistor2013discovering}. Developers tend to see these bugs when a new feature is implemented for the first time, affecting the application’s overall performance. Usually, performance bugs tend to require more time and more experienced developers to fix them; hence, they require more resource utilization and higher costs \cite{bermbach2017quality}. Performance regressions mean that even if the performance of the application is meeting the requirements, it is lower than its previous version that previously performed well; for example, an update or a code change can lead to having a lower time response than before, or there is an increase of resource utilization \cite{nguyen2014industrial}. Both performance bugs and regressions can highly impact the application’s user experience and interaction; therefore, developers must address performance issues early in the development cycle \cite{chen2017exploratory, foo2010mining}. If developers detect a performance bug or regression late, the costs, resources, and efforts required to fix it are higher \cite{grambow2021usingApplication}. For this reason, proactively identifying and solving these issues early enough avoids bigger failures, costs, and waste of resources and reduces long-term maintenance costs by preventing future failures in production \cite{nistor2013discovering, shang2015automatedDetection}. \\
Ali et al. \cite{ali2022performance} argue that testing ensures quality and control throughout development. Nowadays, performance issues can be proactively detected when properly testing the application’s performance before deployment. Developers can perform specific tests to measure performance and identify and resolve issues during the development cycle \cite{ali2022performance}. A performance test mimics the actual application’s performance in a real-case scenario, with underlying potential problems, to solve them before deploying an application; it aims to validate the \ac{SUT} reliability and stability \cite{ali2022performance}. Performance tests enable developers to make the necessary changes without harming the application’s behavior and overall performance. Thus, end-users will not be affected \cite{alghmadi2016automated, chen2017exploratory, foo2010mining}. In this way, performance testing helps detect if any committed code changes may have affected the application’s performance and evolution before releasing a new version of the system \cite{alshoaibi2019DetectionofPerformance, shang2015automatedDetection}.  \\
One of the most common methods of measuring an application’s performance is benchmark suites, a set of standardized tests designed to measure the performance of a system under various conditions \cite{alghamdi2023towards}. The benchmark suites’ objective is to assess an application’s performance under different workloads and conditions, ensuring it meets the performance requirements \cite{alghamdi2023towards, japke2023earlymicrobenchmarkcatches}.These suites aim to stress a system under test, further referred to as \ac{SUT}, with artificial loads that are as real as possible to a real scenario while observing its response using quality metrics, comparing the results with another system or even a previous version to detect performance changes early in time to avoid bigger failures \cite{alghamdi2023towards, japke2023earlymicrobenchmarkcatches, grambow, grambow2021usingApplication, varghese2021survey, folkerts2012benchmarking}. Nowadays, benchmarks are used throughout the different phases of an application life cycle. They are commonly used in the application’s development, implementation, and maintenance to test performance changes \cite{tsuji2017performanceprojection} and deployment. \\
Benchmark suites help developers mitigate the unpredictability of the cloud infrastructure, allowing them to perform repeatable tests in a controlled scenario \cite{japke2023earlymicrobenchmarkcatches}. Although running benchmarks after every code change will enable developers to identify potential performance issues quickly, it is not feasible due to the time it takes to run the performance benchmarks and the associated costs \cite{grambow, alshoaibi2019DetectionofPerformance, he2019statistics}. On top of that, it also delays further application development as developers will need to wait for the results of every benchmark suite to detect a performance change, which increases the amount of required resources like time, costs, and infrastructure \cite{alshoaibi2019DetectionofPerformance, grambow, grambow2021usingApplication, he2019statistics}. The developer is responsible for identifying the best and optimal time to run the benchmarks to obtain meaningful results while balancing efficiency and effectiveness \cite{de2017perphecyperformance}. The benchmark results help developers identify bottlenecks or the root cause of problems, enabling them to evaluate and optimize changes to boost the application’s performance while minimizing resource use and costs \cite{alshoaibi2019DetectionofPerformance, he2019statistics}. Moreover, it gives developers insights into the performance data to make well-informed decisions. \\
Cloud computing has revolutionized how we currently deploy and test applications. The aim is to provide a familiar and integrated platform that multiple users can use simultaneously \cite{hwang2015cloudperformance}. Its infrastructure as a service model and its variety of cloud services have made it popular for performance testing \cite{scheuner2018cloudBenchmarkingSuit, scheuner2018estimatingCloudApplication}. Cloud environments offer flexibility, scalability, reduced infrastructure costs, and simplified maintenance, making them attractive for performance testing. They provide all its resources, such as storage, computing, and networking over the internet \cite{vazquez2014cloudBenchmarkSurvey, borhani2014wpress, langer2011virtualmachineperformancebenchmarking}. It is composed of virtual machines that offer several resources like memory, storage, \ac{CPU}, machine architecture, and \ac{I/O} performance to its users \cite{gillam2013fair}. Given that these resources are available, it is no longer necessary to run the applications in bare metal hardware; instead, they can be deployed in the cloud \cite{he2019statistics, iosup2011performanceVariability}. Thus, cloud computing reduces the need for companies to keep and maintain expensive computing hardware \cite{iosup2011performanceVariability}. Nonetheless, not everything is perfect when deploying to cloud environments. Cloud computing provides unlimited resources for users to generate tests and run simulations. However, using those resources carefully and efficiently is still necessary to reduce costs when performing tests \cite{snellman2011towardsAutomatic}. \\
Furthermore, cloud environments can offer a wide variety of virtual machines (\ac{VM}), and each of them meets different requirements and trade-offs for applications \cite{scheuner2018estimatingCloudApplication, varghese2016cloudbenchmarkingformaximising}. Thus, developers must choose the best \ac{VM} that maximizes their application and performance goals \cite{yadwadkar2017selectingthebest, varghese2016cloudbenchmarkingformaximising}. It is essential to consider that in cloud environments, developers lower the control they have over the software, and they can have shared hardware between different cloud consumers, leading to their application not having access to the complete resources of the virtual machine, affecting the measurements of performance by making it challenging to detect accurate results \cite{he2019statistics, laaber2019softwaremicrobenchmarkinginthecloud, xiong2013Anautomatedmodeldrivenframework} and scale to meet the end-users requests \cite{vazquez2014cloudBenchmarkSurvey, folkerts2012benchmarking}. \\
Therefore, testing that covers uncertain factors is required to provide accurate performance results \cite{he2019statistics}. Benchmarking the performance multiple times and for extended periods is necessary so that the findings do not rely on a single instance and the outcomes can be perceived throughout the experiment. However, limitations have to be considered, as there is a trade-off between costs and execution time \cite{bulej2019initial, laaber2021applyingtcp, mostafa2017perfrankerprioritization}. This trade-off is one of the biggest challenges when addressing performance issues, as developers do not know precisely how long a performance test has to be run to detect issues or changes in its performance \cite{alghmadi2016automated, alghamdi2023towards}. Hence, having a short execution time might result in fewer costs but might lead to underperformance and inaccuracy in the performance metrics collected. However, extending the testing time might produce more accurate results. Still, it comes at the expense of higher costs and computer resources\cite{alghamdi2023towards, he2019statistics, bulej2019initial, bulej2020duet, grambow2020benchmarkingMicroservicesperformance, laaber2021applyingtcp}. For this reason, developers need to be careful when deciding the most cost-effective solution and how long they want to run the experiments to get the best estimation of how performant the software will be so they can minimize unnecessary costs and waste of resources while still offering reliable insights into the performance of the application to make data-driven decisions, updates or changes in the application. \\
Developers have two benchmarking options for measuring and testing an application’s performance: microbenchmarks and application benchmarks. Microbenchmarks test specific function’s performance and must be run repeatedly. As microbenchmarks are very specific and focus only on a particular function, it is hard to know whether a change in this benchmark will impact the overall application’s performance in the real world \cite{bulej2019initial, grambow, laaber2022multi}. On the other hand, an application benchmark is more robust and complex than microbenchmarks since it needs to bring up the whole application into a testable state, which adds much complexity considering multiple factors such as machine configuration, hardware, and network \cite{japke2023earlymicrobenchmarkcatches}.Therefore, running the application benchmark as often as every commit would not be feasible due to the long runtimes, multiple repetitions, resources, and costs it conveys \cite{grambow, japke2023earlymicrobenchmarkcatches, bulej2019initial}. Together, microbenchmarks and application benchmarks enable developers to design, test, and optimize applications in the cloud to detect issues in advance and maximize their performance and reliability. In their paper, Grambow et al. \cite{grambow} stated that detecting an application’s performance is possible using microbenchmark suites. Hence, after considering this study, this thesis aims to establish a correlation between the performance change detected in a microbenchmark and its impact on the application’s benchmark performance using another method. \\
This thesis research question is to what extent can the microbenchmarks predict a slow-down in the application benchmarks’ performance? By doing so, it seeks to enable developers to use microbenchmark results as early indicators of broader performance trends, applying different methodologies that will lead to reality results. The motivation behind this project is to develop a stable and well-proven correlation between the microbenchmark results and application benchmark that saves time and decreases the number of non-performant production releases, giving developers early feedback on the performance changes. 